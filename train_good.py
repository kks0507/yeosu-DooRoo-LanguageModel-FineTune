from getpass import getpass
from huggingface_hub import login
import torch
import ast
import json
from unsloth import FastModel
from unsloth.chat_templates import get_chat_template
from datasets import load_dataset, concatenate_datasets
from trl import SFTTrainer
from unsloth.chat_templates import train_on_responses_only
from transformers import TrainingArguments
from unsloth import FastLanguageModel
import shutil
import os

from unsloth import FastLanguageModel
import torch
from datasets import load_dataset
# 'SFTConfig' ëŒ€ì‹  'TrainingArguments'ë¥¼ 'transformers'ì—ì„œ ê°€ì ¸ì˜µë‹ˆë‹¤.
from transformers import TrainingArguments, TextStreamer
# 'SFTConfig'ëŠ” ë” ì´ìƒ ì—¬ê¸°ì„œ importí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
from trl import SFTTrainer
# =================================================================================
# 1. ì‚¬ìš©ì ì •ë³´ ì„¤ì •
# âš ï¸ ì£¼ì˜: ì•„ë˜ í† í°ì´ í¬í•¨ëœ ì½”ë“œë¥¼ ì™¸ë¶€ì— ê³µìœ í•˜ì§€ ë§ˆì„¸ìš”.
# =================================================================================
HF_TOKEN = "YOUR_HF_TOKEN"  # Hugging Face í† í° ì…ë ¥
MODEL_REPO = "kingkim/Dooroo2025"   # í•™ìŠµ ê²°ê³¼(ëª¨ë¸)ë¥¼ ì—…ë¡œë“œí•  ì €ì¥ì†Œ

# =================================================================================
# 2. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¶ˆëŸ¬ì˜¤ê¸°
# =================================================================================
max_seq_length = 2048
model, tokenizer = FastModel.from_pretrained(
    model_name = "unsloth/Qwen3-4B-Instruct-2507",
    max_seq_length = max_seq_length,
    dtype=torch.bfloat16,
    load_in_4bit = False,  # 4 bit quantization to reduce memory
    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory
    full_finetuning = False, # [NEW!] We have full finetuning now!
    # token = "hf_...", # use one if using gated models
)

# =================================================================================
# 3. LoRA ì„¤ì • (PEFT)
# =================================================================================
model = FastModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 32,
    lora_dropout = 0.05, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)

# =================================================================================
# 4. ë°ì´í„°ì…‹ ì¤€ë¹„
# =================================================================================
import json
from unsloth.chat_templates import get_chat_template
from datasets import load_dataset

# 1) Tokenizerì— Qwen3 ëª¨ë¸ì˜ ê³µì‹ ì±„íŒ… í…œí”Œë¦¿ ì ìš©
print("Tokenizerì— Qwen3 ì±„íŒ… í…œí”Œë¦¿ì„ ì ìš©í•©ë‹ˆë‹¤...")
tokenizer = get_chat_template(
    tokenizer,
    chat_template="qwen3-instruct",
)

# 2) ë°ì´í„°ì…‹ ë¡œë“œ
print("ë°ì´í„°ì…‹ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
print("ë°ì´í„°ì…‹ 'kingkim/yeosu_tour'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
dataset_tour = load_dataset("kingkim/yeosu_tour", token=HF_TOKEN)
print("ë°ì´í„°ì…‹ 'kingkim/yeosu_island'ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
dataset_island = load_dataset("kingkim/yeosu_island", token=HF_TOKEN)

# 3) âœ¨ 'train'ê³¼ 'test' ìŠ¤í”Œë¦¿ì„ ê°ê° í•©ì¹©ë‹ˆë‹¤.
# train ìŠ¤í”Œë¦¿ í•©ì¹˜ê¸°
combined_train_dataset = concatenate_datasets(
    [dataset_tour["train"], dataset_island["train"]]
)
# test ìŠ¤í”Œë¦¿ í•©ì¹˜ê¸°
combined_eval_dataset = concatenate_datasets(
    [dataset_tour["test"], dataset_island["test"]]
)

# 4) âœ¨ (ë§¤ìš° ì¤‘ìš”) í•©ì³ì§„ í•™ìŠµ ë°ì´í„°ì…‹ì„ ì„ì–´ì¤ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•´ì•¼ ëª¨ë¸ì´ ë‘ ì£¼ì œì˜ ë°ì´í„°ë¥¼ ê³¨ê³ ë£¨ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
combined_train_dataset = combined_train_dataset.shuffle(seed=42)

# 5) âœ¨ í•©ì³ì§„ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ë„ë¡ ë³€ìˆ˜ ì´ë¦„ì„ ë§ì¶°ì¤ë‹ˆë‹¤.
from datasets import DatasetDict
dataset = DatasetDict({
    "train": combined_train_dataset,
    "test": combined_eval_dataset
})

# 6) 'messages'ë¥¼ ì•ˆì „í•˜ê²Œ íŒŒì‹±í•˜ì—¬ chat template ì ìš©
def formatting_prompts_func(examples):
    texts = []
    for item in examples["messages"]:
        try:
            # ë¬¸ìì—´ì´ë©´ JSONìœ¼ë¡œ íŒŒì‹±, ì´ë¯¸ list/dictë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
            if isinstance(item, str):
                obj = json.loads(item)
            else:
                obj = item

            # {"messages": [...]} ë˜ëŠ” ë°”ë¡œ [...] í˜•íƒœ ëª¨ë‘ ì§€ì›
            if isinstance(obj, dict) and "messages" in obj:
                msgs = obj["messages"]
            else:
                msgs = obj  # listë¡œ ê°€ì •

            # ìµœì¢… text ìƒì„±
            text = tokenizer.apply_chat_template(
                msgs, tokenize=False, add_generation_prompt=False
            )
        except Exception as e:
            print(f"ë°ì´í„° íŒŒì‹± ì˜¤ë¥˜: {e} â€” ë¹ˆ ë¬¸ìì—´ë¡œ ëŒ€ì²´")
            text = ""  # ë°°ì¹˜ ê¸¸ì´ ë³´ì¡´

        texts.append(text)

    return {"text": texts}

# (ì„ íƒ) ìƒ˜í”Œ íƒ€ì… ë¹ ë¥´ê²Œ ì ê²€ â€” ë§µí•‘ ì „ 1íšŒë§Œ
try:
    sample = dataset["train"][0]["messages"]
    print("ìƒ˜í”Œ íƒ€ì… í™•ì¸:", type(sample))
    print("ìƒ˜í”Œ ê°’(ì•ë¶€ë¶„):", str(sample)[:200])
except Exception as _:
    pass

# 7) ì „ì²´ ìŠ¤í”Œë¦¿ì— í¬ë§·íŒ… ì ìš©
print("ë°ì´í„°ì…‹ì„ ëª¨ë¸ í•™ìŠµ í˜•ì‹ì— ë§ê²Œ ë³€í™˜í•©ë‹ˆë‹¤...")
remove_cols = [c for c in ["messages"] if c in dataset["train"].column_names]
dataset = dataset.map(
    formatting_prompts_func,
    batched=True,
    remove_columns=remove_cols,   # ì¡´ì¬í•  ë•Œë§Œ ì œê±°
)

# 8) ë¹ˆ ë¬¸ìì—´ ì œê±° (íŒŒì‹± ì‹¤íŒ¨ ìƒ˜í”Œ ì •ë¦¬)
dataset = dataset.filter(lambda x: x["text"] != "")

# 9) ìŠ¤í”Œë¦¿ ì¤€ë¹„ (validation ì´ë¦„ ì£¼ì˜)
train_dataset = dataset["train"]
eval_dataset  = dataset["test"]

print("ë°ì´í„°ì…‹ ì¤€ë¹„ ì™„ë£Œ:")
print(f"Train ìƒ˜í”Œ (í•™ìŠµìš©): {len(train_dataset)}")
print(f"Eval ìƒ˜í”Œ (ê²€ì¦ìš©): {len(eval_dataset)}")

print("\ní¬ë§·íŒ…ëœ ìƒ˜í”Œ ì˜ˆì‹œ:")
if len(train_dataset) > 0:
    print(train_dataset[0]["text"])
else:
    print("âš ï¸ train ë°ì´í„°ê°€ ë¹„ì—ˆìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ 'ìƒ˜í”Œ íƒ€ì… í™•ì¸' ë¡œê·¸ì™€ ì›ë³¸ ë°ì´í„°ì˜ 'messages' í•„ë“œë¥¼ ì ê²€í•˜ì„¸ìš”.")

# =================================================================================
# 5. ëª¨ë¸ í•™ìŠµ (Unsloth ìµœì í™” ì ìš©)
# =================================================================================
from trl import SFTTrainer
from transformers import TrainingArguments
from unsloth.chat_templates import train_on_responses_only

# -- SFTTrainerë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
# ì „ë¬¸ê°€ ì½”ë“œì˜ TrainingArguments ë°©ì‹ì„ ì‚¬ìš©í•˜ë˜, ì‚¬ìš©ìë‹˜ì˜ í‰ê°€ ì „ëµì„ ê²°í•©í•©ë‹ˆë‹¤.
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,    # âœ¨ train_datasetìœ¼ë¡œ ëª…í™•íˆ ì§€ì •
    eval_dataset=eval_dataset,      # âœ¨ eval_dataset ì¶”ê°€
    dataset_text_field="text",
    max_seq_length=max_seq_length,
    dataset_num_proc=2,
    packing=False,
    # 'SFTConfig'ë¥¼ 'TrainingArguments'ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. ë‚´ìš©ì€ ë™ì¼í•©ë‹ˆë‹¤.
    args=TrainingArguments(
        per_device_train_batch_size=32,
        gradient_accumulation_steps=2,
        warmup_steps=10,
        num_train_epochs=200,
        learning_rate=4e-6,
        bf16=True,
        fp16=False,
        logging_steps=1,
        optim="adamw_8bit",
        weight_decay=0.01,
        lr_scheduler_type="linear",
        seed=3407,
        output_dir="./outputs",
        report_to="none",
    ),
)

# -- [Unsloth ìµœì í™”] ëª¨ë¸ì´ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì•„ë‹Œ 'ë‹µë³€' ë¶€ë¶„ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ í•™ìŠµí•˜ë„ë¡ ì„¤ì •í•©ë‹ˆë‹¤.
# ì´ëŠ” ëª¨ë¸ì´ ì§ˆë¬¸ í˜•ì‹ì„ ëª¨ë°©í•˜ëŠ” ëŒ€ì‹ , ë‹µë³€ ìƒì„± ëŠ¥ë ¥ì„ ë†’ì´ëŠ” ë° íš¨ê³¼ì ì…ë‹ˆë‹¤.
from unsloth.chat_templates import train_on_responses_only
trainer = train_on_responses_only(
    trainer,
    instruction_part = "<|im_start|>user\n",
    response_part = "<|im_start|>assistant\n",
)

# -- ëª¨ë¸ í•™ìŠµ ì‹œì‘ --
print("\nëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
trainer_stats = trainer.train()
print("ëª¨ë¸ íŒŒì¸íŠœë‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

# =================================================================================
# 6. ëª¨ë¸ í‰ê°€ ë° ì €ì¥ (ìˆ˜ì •ëœ ìµœì¢…ì•ˆ)
# =================================================================================
# 1. ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ì„ ëª…ì‹œì ìœ¼ë¡œ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
#    - trainer.evaluate()ë¥¼ í˜¸ì¶œí•˜ë©´ SFTTrainer ì„¤ì • ì‹œ ì „ë‹¬í•œ eval_datasetìœ¼ë¡œ í‰ê°€ê°€ ìˆ˜í–‰ë©ë‹ˆë‹¤.
print("\nìµœì¢… ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤...")
eval_results = trainer.evaluate()

print("ìµœì¢… í‰ê°€ ê²°ê³¼:")
print(eval_results)

# --- ğŸ‘‡ [ì¶”ê°€] í‰ê°€ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ ---
import json
with open("evaluation_results.json", "w") as f:
    json.dump(eval_results, f, indent=4)
print("\nâœ… í‰ê°€ ê²°ê³¼ë¥¼ 'evaluation_results.json' íŒŒì¼ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

# 3. í•™ìŠµëœ ìµœì¢… LoRA ì–´ëŒ‘í„°ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
# final_adapter_path = "./final_adapter"
# print(f"\ní•™ìŠµëœ LoRA ì–´ëŒ‘í„°ë¥¼ '{final_adapter_path}' ê²½ë¡œì— ì €ì¥í•©ë‹ˆë‹¤...")
# trainer.save_model(final_adapter_path)
# print("ì €ì¥ ì™„ë£Œ.")

# =================================================================================
# 7. LoRA ì–´ëŒ‘í„° ì ìš©í•˜ì—¬ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ê²°í•©
# =================================================================================
# Hugging Face Hubì— ìƒì„±ë  ë ˆí¬ì§€í† ë¦¬ ì •ë³´
hf_repo_id = MODEL_REPO # ë³€ê²½ í•„ìš”

# --- 1. ì €ì¥ëœ LoRA ì–´ëŒ‘í„°ë¥¼ ë‹¤ì‹œ ë¡œë“œí•©ë‹ˆë‹¤ ---
# print(f"'{final_adapter_path}'ì—ì„œ ì–´ëŒ‘í„°ë¥¼ ë¡œë“œí•˜ê³  ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ê²°í•©í•©ë‹ˆë‹¤...")
print("\nLoRA ì–´ëŒ‘í„°ë¥¼ ë² ì´ìŠ¤ ëª¨ë¸ê³¼ ë³‘í•©í•©ë‹ˆë‹¤...")
# ì´ ì‹œì ì˜ model ê°ì²´ëŠ” í•™ìŠµì´ ì™„ë£Œëœ LoRA ëª¨ë¸ì…ë‹ˆë‹¤.
model = model.merge_and_unload()
print("ë³‘í•© ì™„ë£Œ.")

# from_pretrainedì— LoRA ì–´ëŒ‘í„° ê²½ë¡œë¥¼ ì§ì ‘ ì§€ì •í•©ë‹ˆë‹¤.
# UnslothëŠ” ì–´ëŒ‘í„° ì„¤ì • íŒŒì¼ì„ ë³´ê³  ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ ë² ì´ìŠ¤ ëª¨ë¸ì„ ë¡œë“œí•œ í›„,
# LoRA ì–´ëŒ‘í„°ë¥¼ ì ìš©í•˜ì—¬ ì™„ë²½í•œ PeftModel ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
# model, tokenizer = FastLanguageModel.from_pretrained(
#    model_name = final_adapter_path, 
#    max_seq_length = 2048,
#    dtype = torch.bfloat16,
#    load_in_4bit = False,
#    load_in_8bit = False,
#)

# --- 4. ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œ ---
# ì´ì œ model ê°ì²´ëŠ” ì–´ëŒ‘í„°ê°€ ì ìš©ëœ ìƒíƒœë¡œ ì˜¬ë°”ë¥´ê²Œ ì¸ì‹ë©ë‹ˆë‹¤.
print(f"ëª¨ë¸ì„ ë³‘í•©í•˜ì—¬ '{hf_repo_id}' ë ˆí¬ì§€í† ë¦¬ì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
print("ì´ ì‘ì—…ì€ ì‹œê°„ì´ ë‹¤ì†Œ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

# 2. ë³‘í•©ëœ ì¼ë°˜ ëª¨ë¸ì´ë¯€ë¡œ, ì¼ë°˜ ì—…ë¡œë“œ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
model.push_to_hub(
    "kingkim/Dooroo2025", # ë ˆí¬ì§€í„°ë¦¬ ID
    tokenizer = tokenizer,
    token = HF_TOKEN,
)

print("\nì—…ë¡œë“œ ì™„ë£Œ!")
print(f"ëª¨ë¸ í˜ì´ì§€: https://huggingface.co/{hf_repo_id}")
