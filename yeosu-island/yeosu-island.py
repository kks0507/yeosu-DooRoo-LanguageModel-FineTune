# requirements:
# pip install pandas datasets huggingface_hub google-generativeai tqdm scikit-learn

import os
import re
import json
import time
import pandas as pd
import google.generativeai as genai
from datasets import Dataset, DatasetDict
from huggingface_hub import HfApi
from tqdm import tqdm
from sklearn.model_selection import train_test_split

# =========================
# 0) ÏÑ§Ï†ï & ÌôòÍ≤Ω
# =========================
GEMINI_API_KEY = "YOUR_API_KEY"  # Î≥∏Ïù∏Ïùò Gemini API ÌÇ§Î°ú ÍµêÏ≤¥ÌïòÏÑ∏Ïöî.
HF_TOKEN       = "YOUR_API_KEY" # Î≥∏Ïù∏Ïùò Hugging Face Ïì∞Í∏∞(write) ÌÜ†ÌÅ∞ÏúºÎ°ú ÍµêÏ≤¥ÌïòÏÑ∏Ïöî.

# Ïã§Ìñâ ÌååÎùºÎØ∏ÌÑ∞
CSV_FILE_PATH   = "YOUR_TRAIN_DATA.csv"
HF_REPO_ID      = "YOUR_HF_USERNAME/YOUR_DATASET_NAME"  # Ïòà: "myusername/myislands-dataset"
MODEL_NAME      = "gemini-2.5-flash-lite-preview-06-17"
N_AUGMENTATIONS = 3
SLEEP_SEC       = 7.0
MAX_ROWS        = None
SYSTEM_PROMPT   = "You are a helpful assistant who is an expert on the islands of Yeosu, South Korea. Please provide kind and accurate answers to the user's questions."

# API ÌÅ¥ÎùºÏù¥Ïñ∏Ìä∏ Ï¥àÍ∏∞Ìôî
if not GEMINI_API_KEY or "YOUR" in GEMINI_API_KEY:
    raise ValueError("GEMINI_API_KEYÎ•º ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî.")
if not HF_TOKEN or "YOUR" in HF_TOKEN:
    raise ValueError("HF_TOKENÏùÑ ÏÑ§Ï†ïÌï¥Ï£ºÏÑ∏Ïöî.")

genai.configure(api_key=GEMINI_API_KEY)
hf_api = HfApi(token=HF_TOKEN)

# =========================
# 1) Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï (Í∞ïÌôîÎêú ÌîÑÎ°¨ÌîÑÌä∏)
# =========================
def paraphrase_question_gemini(model: genai.GenerativeModel, island: str, question: str, n: int) -> list[str]:
    """
    GeminiÎ•º Ìò∏Ï∂úÌïòÏó¨ Îã§ÏñëÌïú Ïä§ÌÉÄÏùºÍ≥º Ïñ¥Ìà¨Ïùò ÏßàÎ¨∏ÏùÑ nÍ∞ú ÏÉùÏÑ±Ìï©ÎãàÎã§.
    """
    prompt = f"""
# ROLE
You are a creative copywriter specializing in generating diverse user questions. Your goal is to create a variety of questions that real people would ask, based on a single, standardized query.

# MISSION
Generate {n} new questions based on the "ORIGINAL QUESTION" below.
The generated questions must ask for the exact same information, but should have completely different tones, styles, and phrasing, as if asked by different people.

# CORE INFORMATION (MUST BE INCLUDED IN EVERY QUESTION)
- Island Name: "{island}"

# ORIGINAL QUESTION
"{question}"

# GUIDELINES FOR DIVERSIFICATION (Generate a mix of styles like these)
1.  **Friendly & Casual (ÏπúÍ∑ºÌïú ÎßêÌà¨):** Like asking a friend. (e.g., "ÎÇ≠ÎèÑ Ïù¥Î¶ÑÏùÄ Î¨¥Ïä® ÎúªÏù¥Ïïº?", "ÌòπÏãú ÎÇ≠ÎèÑ Ï£ºÏÜå ÏïåÏïÑ?")
2.  **Direct & Concise (ÏßÅÏ†ëÏ†ÅÏù¥Í≥† Í∞ÑÍ≤∞Ìïú ÎßêÌà¨):** Getting straight to the point. (e.g., "ÎÇ≠ÎèÑ Î©¥Ï†Å?", "ÎÇ≠ÎèÑ ÌäπÏÇ∞Î¨º ÏïåÎ†§Ï§ò.")
3.  **Polite & Formal (Ï†ïÏ§ëÌïú ÎßêÌà¨):** As if asking in a formal setting. (e.g., "ÎÇ≠ÎèÑÏùò Ïù¥Î¶ÑÏù¥ Ïú†ÎûòÎêú Î∞∞Í≤ΩÏóê ÎåÄÌï¥ ÏÑ§Î™ÖÌï¥Ï£ºÏã§ Ïàò ÏûàÎÇòÏöî?")
4.  **Inquisitive & Detailed (Ï°∞Í∏à Îçî ÏÉÅÏÑ∏ÌïòÍ≤å Î¨ªÎäî ÎßêÌà¨):** Asking with more curiosity. (e.g., "ÎÇ≠ÎèÑÎùºÎäî ÏÑ¨ÏùÄ Ïôú Í∑∏Îü∞ Ïù¥Î¶ÑÏù¥ Î∂ôÍ≤å Îêú Í±¥ÏßÄ Í∂ÅÍ∏àÌï¥Ïöî.")
5.  **Beginner's Question (Ïó¨ÌñâÍ∞ùÏù¥ÎÇò Ï¥àÏã¨ÏûêÏùò ÏßàÎ¨∏):** As if asking for the first time. (e.g., "Ïó¨Ïàò ÎÇ≠ÎèÑÏóê Í∞ÄÎ†§Î©¥ Î∞∞ Ïñ¥ÎîîÏÑú ÌÉÄÏöî?", "ÎÇ≠ÎèÑÏóê Í∞ÄÎ©¥ Íº≠ Î¥êÏïº Ìï† Í≤å Î≠îÍ∞ÄÏöî?")

# CONSTRAINTS
- ALWAYS maintain the original question's core intent. Do not ask for different information.
- ALWAYS end the sentence with a question mark (?).
- DO NOT simply reorder the words from the original question. Create genuinely new sentences.
- Output ONLY a single JSON array containing exactly {n} question strings. Do not include any other text, explanations, or formatting.
- Example output format: ["ÏßàÎ¨∏1", "ÏßàÎ¨∏2", "ÏßàÎ¨∏3"]
"""
    for attempt in range(3):
        try:
            resp = model.generate_content(prompt)
            raw_text = (getattr(resp, "text", None) or "").strip()
            match = re.search(r'\[.*\]', raw_text, re.DOTALL)
            if not match:
                raise ValueError("JSON array not found in the response.")
            paraphrases = json.loads(match.group(0))
            if isinstance(paraphrases, list) and len(paraphrases) >= n:
                return [str(p).strip() for p in paraphrases[:n]]
            else:
                raise ValueError(f"Not enough paraphrases generated ({len(paraphrases)} < {n}).")
        except Exception as e:
            print(f"  - (Warning) Paraphrase generation failed (Attempt {attempt+1}/3): {e}")
            if attempt < 2:
                time.sleep(SLEEP_SEC * (attempt + 1))
            else:
                print(f"  - (Failed) Returning original question after final attempt: {question}")
                return [question] * n
    return [question] * n

# =========================
# 2) Îç∞Ïù¥ÌÑ∞ Ìè¨Îß∑ÌåÖ Î∞è Î∂ÑÌï†
# =========================
def to_qwen_chat_format(question: str, answer: str, system_prompt: str) -> dict:
    return {
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": str(question).strip()},
            {"role": "assistant", "content": str(answer).strip()}
        ]
    }

# =========================
# 3) Î©îÏù∏ ÌååÏù¥ÌîÑÎùºÏù∏
# =========================
def main():
    print(f"[Load] Reading CSV file: {CSV_FILE_PATH}")
    df_orig = pd.read_csv(CSV_FILE_PATH)
    
    df_orig.columns = [col.lower().strip() for col in df_orig.columns]
    required_cols = ['island', 'question', 'answer']
    if not all(col in df_orig.columns for col in required_cols):
        raise KeyError(f"CSV must contain the following columns: {required_cols}")

    df_orig = df_orig.dropna(subset=required_cols).copy()
    for col in required_cols:
        df_orig[col] = df_orig[col].astype(str).str.strip()
    df_orig = df_orig[df_orig["question"] != ""].reset_index(drop=True)

    if MAX_ROWS:
        df_orig = df_orig.head(MAX_ROWS)
        print(f"[Note] Dev Mode: Processing only the first {len(df_orig)} rows.")

    print(f"\n[Split] Splitting {len(df_orig)} original samples into Dev/Test sets...")
    test_rows = [
    to_qwen_chat_format(row['question'], row['answer'], SYSTEM_PROMPT)
    for _, row in df_orig.iterrows()
    ]

    print(f"\n[Augment] Creating Train set by paraphrasing {len(df_orig)} questions ({N_AUGMENTATIONS} each)...")
    model = genai.GenerativeModel(MODEL_NAME)
    train_rows = []
    failed_paraphrases = 0

    for _, row in tqdm(df_orig.iterrows(), total=len(df_orig), desc="Augmenting Train Data"):
        paraphrased_qs = paraphrase_question_gemini(
            model,
            island=row['island'],
            question=row['question'],
            n=N_AUGMENTATIONS
        )
        for pq in paraphrased_qs:
            if pq.strip() == row['question'].strip():
                failed_paraphrases += 1
            train_rows.append(to_qwen_chat_format(pq, row['answer'], SYSTEM_PROMPT))
        time.sleep(SLEEP_SEC)

    print(f"[Result] Generated {len(train_rows)} augmented samples for Train set.")
    print(f"  - Failed/identical paraphrases: {failed_paraphrases} out of {len(df_orig) * N_AUGMENTATIONS}")

    unique_train_rows_str = {json.dumps(d, sort_keys=True) for d in train_rows}
    train_rows = [json.loads(s) for s in unique_train_rows_str]
    print(f"  - Train samples after deduplication: {len(train_rows)}")

    if not train_rows or not test_rows:
        raise RuntimeError("Data creation failed. Cannot upload empty datasets.")

    dsd = DatasetDict({
        "train": Dataset.from_list(train_rows),
        "test": Dataset.from_list(test_rows)
    })
    
    print("\n[Hugging Face] Generated Dataset Information:")
    print(dsd)

    print(f"\n[Upload] Pushing dataset to https://huggingface.co/datasets/{HF_REPO_ID}")
    try:
        dsd.push_to_hub(HF_REPO_ID, token=HF_TOKEN)
        print("‚úÖ All tasks completed successfully!")
    except Exception as e:
        print(f"üö® Failed to upload to Hugging Face Hub: {e}")

if __name__ == "__main__":
    main()